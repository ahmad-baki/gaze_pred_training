# https://wandb.ai/ahmad-baki-karlsruhe-institute-of-technology/gaze_pred_training/runs/2nmnace5/

defaults:
  - models@model: gaze_pre
  - dataset@dataset: gaze_dataset_workspace_pear_banana_in_sink

wandb:
  project: test # gaze_pred_training        # replace with your project name
  entity: ahmad-baki-karlsruhe-institute-of-technology  # replace with your WandB entity

# ────────────────────────────────────────────────────────
# General training settings
model_save_interval: 10      # save model checkpoint every n epochs
epochs: 300
top_k: 3
dropout: 0.18279632941608848
batch_size: 512
# note: code now uses cfg.lr, not `learning_rate`
lr: !!float 5.751833600952328e-06
weight_decay: !!float 1.8820141960152387e-05

# ────────────────────────────────────────────────────────
# Optimizer
optimizer:
  # one of: adam, adamw, sgd, ranger
  type: adam
  # these will only be used by Adam/AdamW
  betas0: 0.9551763556721846
  betas1: 0.930329412625476

  # for SGD
  momentum: 0.8840302508317566
  
# ────────────────────────────────────────────────────────
# Learning‐rate Scheduler
scheduler:
  # one of: steplr, cosine, onecycle, reduceonplateau
  type: onecycle
  
  # for StepLR
  step_size: 5
  gamma: 0.2359669783993054
  
  # only for ReduceLROnPlateau
  patience: 5
  
  # only for OneCycleLR
  max_lr: !!float 3.794373153182155e-05

# ────────────────────────────────────────────────────────
# Loss & regularization
# label smoothing is applied in CrossEntropyLoss
# label_smoothing: 0.0

# ────────────────────────────────────────────────────────
# Gradient clipping
gradient_clip_max_norm: 1.0

sampler: random_sampler # weighted_random_sampler

# only used for weighted_random_sampler
alpha: 0.2914457512154989

# ────────────────────────────────────────────────────────
# Loss instantiation
loss:
  _target_: torch.nn.CrossEntropyLoss
